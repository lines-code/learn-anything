{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c9a2178",
   "metadata": {},
   "source": [
    "# Langgraph ê¸°ë°˜ ë‹¤ë‹¨ê³„ ë¬¸ì„œ-ê·¸ë˜í”„ êµ¬ì¶• ì˜ˆì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b01d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install openai langchain python-arango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph ê¸°ë°˜ ë‹¤ë‹¨ê³„ ë¬¸ì„œ-ê·¸ë˜í”„ êµ¬ì¶• ì˜ˆì œ\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from arango import ArangoClient\n",
    "import json\n",
    "\n",
    "# 1. Document Loader\n",
    "\n",
    "def load_document(state):\n",
    "    doc_path = state[\"doc_path\"]\n",
    "    page = state.get(\"page\", 0)\n",
    "    loader = PyPDFLoader(doc_path)\n",
    "    pages = loader.load()\n",
    "    if page >= len(pages):\n",
    "        return {\"end\": True}\n",
    "    return {\"text\": pages[page].page_content, \"page\": page, \"end\": False}\n",
    "\n",
    "# 2. Entity Extraction\n",
    "\n",
    "ENTITY_PROMPT = PromptTemplate.from_template(\"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ nodesì™€ edgesë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ê²°ê³¼ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n",
    "\n",
    "ë¬¸ì„œ:\n",
    "\"\"\"{text}\"\"\"\n",
    "\n",
    "ê²°ê³¼ ì˜ˆì‹œ:\n",
    "{\n",
    "  \"nodes\": [\n",
    "    {\"_key\": \"steve_jobs\", \"name\": \"Steve Jobs\", \"type\": \"person\"},\n",
    "    {\"_key\": \"apple\", \"name\": \"Apple\", \"type\": \"company\"}\n",
    "  ],\n",
    "  \"edges\": [\n",
    "    {\"_from\": \"nodes/steve_jobs\", \"_to\": \"nodes/apple\", \"relation\": \"founded\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "extract_chain = LLMChain(llm=llm, prompt=ENTITY_PROMPT)\n",
    "\n",
    "def extract_entities_with_llm(state):\n",
    "    result = extract_chain.run(text=state[\"text\"])\n",
    "    data = json.loads(result)\n",
    "    return {\"nodes\": data[\"nodes\"], \"edges\": data[\"edges\"], \"page\": state[\"page\"], \"doc_path\": state[\"doc_path\"]}\n",
    "\n",
    "# 3. Validation (ê°„ë‹¨íˆ ì¤‘ë³µ í•„í„°ë§ë§Œ ì˜ˆì‹œ)\n",
    "\n",
    "def validate_entities(state):\n",
    "    seen_keys = set()\n",
    "    nodes = []\n",
    "    for n in state[\"nodes\"]:\n",
    "        if n[\"_key\"] not in seen_keys:\n",
    "            seen_keys.add(n[\"_key\"])\n",
    "            nodes.append(n)\n",
    "    return {**state, \"nodes\": nodes}\n",
    "\n",
    "# 4. ArangoDB ì‚½ì…\n",
    "\n",
    "def insert_to_arango(state):\n",
    "    db = ArangoClient().db(\"_system\", username=\"root\", password=\"your_password\")\n",
    "    if not db.has_collection(\"nodes\"):\n",
    "        db.create_collection(\"nodes\")\n",
    "    if not db.has_collection(\"edges\"):\n",
    "        db.create_collection(\"edges\", edge=True)\n",
    "    nc = db.collection(\"nodes\")\n",
    "    ec = db.collection(\"edges\")\n",
    "\n",
    "    for n in state[\"nodes\"]:\n",
    "        if not nc.has(n[\"_key\"]):\n",
    "            nc.insert(n)\n",
    "    for e in state[\"edges\"]:\n",
    "        ec.insert(e)\n",
    "\n",
    "    return {**state, \"page\": state[\"page\"] + 1}\n",
    "\n",
    "# 5. ì¡°ê±´ ë¶„ê¸°\n",
    "\n",
    "def check_more_pages(state):\n",
    "    return \"end\" if state.get(\"end\") else \"continue\"\n",
    "\n",
    "# 6. LangGraph ì •ì˜\n",
    "\n",
    "graph = StateGraph()\n",
    "graph.add_node(\"Load\", load_document)\n",
    "graph.add_node(\"Extract\", extract_entities_with_llm)\n",
    "graph.add_node(\"Validate\", validate_entities)\n",
    "graph.add_node(\"Save\", insert_to_arango)\n",
    "\n",
    "graph.set_entry_point(\"Load\")\n",
    "graph.add_edge(\"Load\", \"Extract\")\n",
    "graph.add_edge(\"Extract\", \"Validate\")\n",
    "graph.add_edge(\"Validate\", \"Save\")\n",
    "graph.add_conditional_edges(\"Save\", check_more_pages, {\n",
    "    \"continue\": \"Load\",\n",
    "    \"end\": \"__end__\"\n",
    "})\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "# 7. ì‹¤í–‰\n",
    "\n",
    "initial_state = {\"doc_path\": \"sample.pdf\", \"page\": 0}\n",
    "workflow.invoke(initial_state)\n",
    "\n",
    "\n",
    "# 8. Streamlit ì‹œê°í™” ëŒ€ì‹œë³´ë“œ\n",
    "\n",
    "def visualize_graph():\n",
    "    st.title(\"ğŸ“Š ArangoDB ì§€ì‹ ê·¸ë˜í”„ ì‹œê°í™”\")\n",
    "    db = ArangoClient().db(\"_system\", username=\"root\", password=\"your_password\")\n",
    "    query = \"\"\"\n",
    "    FOR v, e IN 1..2 ANY 'nodes/steve_jobs' GRAPH 'knowledge_graph'\n",
    "      RETURN { vertex: v, edge: e }\n",
    "    \"\"\"\n",
    "    cursor = db.aql.execute(query)\n",
    "    data = list(cursor)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for item in data:\n",
    "        v = item['vertex']\n",
    "        e = item['edge']\n",
    "        from_key = e['_from'].split('/')[1]\n",
    "        to_key = e['_to'].split('/')[1]\n",
    "        relation = e.get('relation', '')\n",
    "        G.add_node(from_key)\n",
    "        G.add_node(to_key)\n",
    "        G.add_edge(from_key, to_key, label=relation)\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"skyblue\", node_size=2000, font_size=12)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10)\n",
    "    st.pyplot(plt)\n",
    "\n",
    "# Streamlit ì‹¤í–‰ (main.pyë¡œ ì €ì¥ í›„ ì‹¤í–‰)\n",
    "# streamlit run main.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
